# Projekt semestralny PRIR

## ğŸ“Œ Opis projektu

Aplikacja umoÅ¼liwia scrapowanie danych o produktach z serwisu **eBay**, zgodnie z parametrami zadanymi przez uÅ¼ytkownika (nazwa produktu, przedziaÅ‚ cenowy). System dziaÅ‚a w architekturze rozproszonej â€“ oparty na trzech moduÅ‚ach: interfejs uÅ¼ytkownika, silnik scrapujÄ…cy oraz baza danych. Wszystkie komponenty dziaÅ‚ajÄ… w osobnych kontenerach Docker.

Projekt wykorzystuje:
- `Flask` â€“ interfejs uÅ¼ytkownika
- `asyncio`, `aiohttp`, `multiprocessing` â€“ silnik scrapujÄ…cy
- `BeautifulSoup` â€“ parsowanie treÅ›ci HTML
- `MongoDB` â€“ baza danych
- `Docker`, `docker-compose` â€“ konteneryzacja

## ğŸ“‚ Struktura projektu

```
Projekt_PRIR/
â”œâ”€â”€ Dockerfile # Instrukcja budowy gÅ‚Ã³wnego kontenera aplikacji
â”œâ”€â”€ docker-compose.yml # Definicje i konfiguracja wszystkich kontenerÃ³w, zarzÄ…dzanie usÅ‚ugami
â”‚
â”œâ”€â”€ engine/ # Silnik scrapujÄ…cy
â”‚   â”œâ”€â”€ Dockerfile # Instrukcja budowy kontenera dla silnika scrapujÄ…cego
â”‚   â”œâ”€â”€ requirements.txt # Lista zaleÅ¼noÅ›ci Python dla silnika
â”‚   â”œâ”€â”€ engine_api.py # API silnika
â”‚   â””â”€â”€ scraper.py # GÅ‚Ã³wna logika scrapowania, przetwarzania danych i interakcji z API
â”‚
â””â”€â”€ flask_app/ # Interfejs uÅ¼ytkownika (Flask)
    â”œâ”€â”€ Dockerfile # Instrukcja budowy kontenera dla aplikacji Flask
    â”œâ”€â”€ requirements.txt # Lista zaleÅ¼noÅ›ci Python dla aplikacji Flask
    â”œâ”€â”€ app.py # GÅ‚Ã³wna logika aplikacji Flask
    â”‚
    â”œâ”€â”€ static/ # Katalog na pliki statyczne
    â”‚   â”‚
    â”‚   â””â”€â”€ css/ # Podkatalog na arkusze stylÃ³w CSS
    â”‚       â”œâ”€â”€ style_index.css # Style CSS dla strony gÅ‚Ã³wnej (index.html)
    â”‚       â””â”€â”€ style_results.css # Style CSS dla strony wynikÃ³w (results.html)
    â”‚
    â””â”€â”€ templates/ # Katalog na szablony HTML uÅ¼ywane przez Flask
        â”œâ”€â”€ index.html # Szablon HTML dla strony gÅ‚Ã³wnej
        â””â”€â”€ results.html # Szablon HTML dla strony wyÅ›wietlajÄ…cej wyniki
```

## ğŸ¯ Zakres zbieranych danych

Dla kaÅ¼dego ogÅ‚oszenia zbierane sÄ… nastÄ™pujÄ…ce dane:
- **title** â€“ tytuÅ‚ ogÅ‚oszenia
- **price_text** â€“ oryginalny tekst z cenÄ…
- **price_value** â€“ przekonwertowana wartoÅ›Ä‡ liczbowo
- **shipping_info** â€“ koszt wysyÅ‚ki
- **location** â€“ lokalizacja przedmiotu
- **link** â€“ bezpoÅ›redni link do ogÅ‚oszenia
- **query_source** â€“ sÅ‚owo kluczowe uÅ¼yte do wyszukiwania

## âš™ï¸ Technologie

- Python 3.10+
- Flask
- BeautifulSoup4
- aiohttp
- asyncio
- multiprocessing
- MongoDB
- Docker + Docker Compose

## ğŸš€ Uruchamianie aplikacji

1. Klonowanie repozytorium:
```bash
git clone https://github.com/Thiago1717/PRIR-Projekt.git
cd PRIR-Projekt
```

2. Budowanie i uruchamianie:
```bash
docker-compose up --build
```

3. Interfejs uÅ¼ytkownika bÄ™dzie dostÄ™pny pod adresem:
```
http://localhost:5000/
```

## ğŸ›ï¸ Architektura i ModuÅ‚y

Aplikacja skÅ‚ada siÄ™ z trzech gÅ‚Ã³wnych, skonteneryzowanych moduÅ‚Ã³w:

1.  **Interfejs UÅ¼ytkownika (`flask_ui_container`):**
    *   Zbudowany przy uÅ¼yciu Flask.
    *   Odpowiedzialny za interakcjÄ™ z uÅ¼ytkownikiem â€“ przyjmowanie parametrÃ³w wyszukiwania (sÅ‚owo kluczowe, ceny, sortowanie) oraz prezentacjÄ™ wynikÃ³w.
    *   Komunikuje siÄ™ z silnikiem scrapujÄ…cym poprzez jego API (`engine_container:5001`).
    *   Pobiera wyniki bezpoÅ›rednio z bazy danych (`mongo_db_container:27017`) w celu ich wyÅ›wietlenia.
    *   DziaÅ‚a na porcie `5000`.

2.  **Silnik ScrapujÄ…cy (`engine_container`):**
    *   Zbudowany rÃ³wnieÅ¼ przy uÅ¼yciu Flask (do wystawienia prostego API).
    *   Zawiera logikÄ™ pobierania danych z eBay (asynchronicznie przy uÅ¼yciu `aiohttp` i `asyncio`) oraz ich przetwarzania (wieloprocesowo przy uÅ¼yciu `multiprocessing` i `BeautifulSoup`).
    *   Po przetworzeniu, zapisuje dane w bazie MongoDB.
    *   DziaÅ‚a na porcie `5001`.

3.  **Baza Danych (`mongo_db_container`):**
    *   Standardowy kontener z obrazem `mongo:latest`.
    *   Przechowuje wyniki scrapowania w bazie danych o nazwie `scraper_db`.
    *   Dla kaÅ¼dego zapytania uÅ¼ytkownika tworzona jest dedykowana kolekcja.
    *   DziaÅ‚a na standardowym porcie MongoDB `27017`.

### ğŸ”„ PrzepÅ‚yw danych miÄ™dzy moduÅ‚ami:

1.  UÅ¼ytkownik wprowadza dane w formularzu na stronie gÅ‚Ã³wnej (`index.html`) w `flask_ui_container`.
2.  Skrypt JavaScript na stronie `index.html` wysyÅ‚a Å¼Ä…danie POST z parametrami do endpointu `/trigger-scrape` w `flask_ui_container`.
3.  Aplikacja Flask w `flask_ui_container` waliduje dane i wysyÅ‚a Å¼Ä…danie POST do API silnika scrapujÄ…cego (`engine_container:5001/start-scraping`).
4.  `engine_container` odbiera Å¼Ä…danie, uruchamia proces scrapowania:
    *   Asynchronicznie pobiera strony HTML z eBay (`fetch_html_content` uÅ¼ywajÄ…c `aiohttp`).
    *   Przekazuje pobrane treÅ›ci HTML do puli procesÃ³w (`ProcessPoolExecutor`) w celu rÃ³wnolegÅ‚ego parsowania (`parse_single_item_html` uÅ¼ywajÄ…c `BeautifulSoup`).
    *   Zapisuje sparsowane dane do odpowiedniej kolekcji w `mongo_db_container`.
5.  `engine_container` zwraca status operacji (JSON) do `flask_ui_container`.
6.  `flask_ui_container` przekazuje odpowiedÅº silnika do skryptu JavaScript na stronie `index.html`, ktÃ³ry wyÅ›wietla komunikat uÅ¼ytkownikowi.
7.  UÅ¼ytkownik moÅ¼e przejÅ›Ä‡ do strony wynikÃ³w (`/results?query=...`), gdzie `flask_ui_container` pobiera dane bezpoÅ›rednio z `mongo_db_container` i renderuje je w szablonie `results.html`.

### ğŸ³ Konfiguracja Docker Compose (`docker-compose.yml`)

Plik `docker-compose.yml` konfiguruje dziaÅ‚anie wszystkich trzech usÅ‚ug:
- Definiuje usÅ‚ugi: `flask_ui`, `engine_app`, `mongo_db`.
- Mapuje porty: `5000:5000` (UI), `5001:5001` (Engine), `27017:27017` (MongoDB).
- Ustawia zaleÅ¼noÅ›ci (`depends_on`), aby zapewniÄ‡ prawidÅ‚owÄ… kolejnoÅ›Ä‡ uruchamiania (np. silnik i UI zaleÅ¼Ä… od bazy danych).
- Definiuje sieÄ‡ `app_network`, umoÅ¼liwiajÄ…cÄ… komunikacjÄ™ miÄ™dzy kontenerami po nazwach usÅ‚ug (np. `http://engine_app:5001`).
- Konfiguruje wolumeny:
    - Dla `flask_ui` i `engine_app` mapuje lokalne foldery kodu do kontenerÃ³w
    - Dla `mongo_db` tworzy nazwany wolumen `mongo_data` do przechowywania danych bazy, nawet po zatrzymaniu/usuniÄ™ciu kontenera.
- Przekazuje zmienne Å›rodowiskowe, np. `MONGO_URI` i `ENGINE_URL`, aby kontenery mogÅ‚y siÄ™ ze sobÄ… komunikowaÄ‡.

## ğŸ“„ Kluczowe fragmenty kodu i ich opis

### ModuÅ‚ Silnika ScrapujÄ…cego (`engine/`)

#### `scraper.py`:
-   **Konfiguracja globalna:**
    *   `MONGO_URI`, `DB_NAME`: dane dostÄ™powe do MongoDB, odczytywane ze zmiennych Å›rodowiskowych.
    *   `USER_AGENTS`: lista User-AgentÃ³w do rotacji, aby zmniejszyÄ‡ ryzyko blokady przez eBay.
-   **`run_ebay_scraper(query, min_price, max_price, sort_order)`:**
    *   GÅ‚Ã³wna funkcja konfigurujÄ…ca scrapowanie.
    *   Tworzy nowÄ… pÄ™tlÄ™ zdarzeÅ„ `asyncio` i `ProcessPoolExecutor`.
    *   Inicjalizuje obiekt `EbayScraper`.
    *   WywoÅ‚uje `scraper.scrape_and_save()` do faktycznego pobrania, przetworzenia i zapisania danych.
    *   ZarzÄ…dza zasobami (zamykanie poÅ‚Ä…czeÅ„, pÄ™tli, executora).
-   **`EbayScraper.__init__(self, loop, executor)`:**
    *   Inicjalizuje scraper, ustawia `base_url` dla eBay.
    *   NawiÄ…zuje poÅ‚Ä…czenie z MongoDB (`get_mongo_client_and_db`).
    *   Przechowuje referencje do pÄ™tli `asyncio` i `ProcessPoolExecutor`.
-   **`EbayScraper.scrape_and_save(self, query, ...)`:**
    *   Przygotowuje parametry Å¼Ä…dania do eBay.
    *   UÅ¼ywa `aiohttp.ClientSession()` do asynchronicznego pobierania stron (`fetch_html_content`).
    *   Wykorzystuje `asyncio.gather` do jednoczesnego wykonania zadaÅ„ pobierania i parsowania.
    *   Parsowanie (`parse_ebay_page_content_multiproc`) delegowane jest do puli procesÃ³w.
    *   Zapisuje wyniki do MongoDB uÅ¼ywajÄ…c `collection.update_one` z `upsert=True` (aktualizuje istniejÄ…ce lub wstawia nowe).
    *   Nazwa kolekcji jest sanitizowana przez `_sanitize_collection_name`.
-   **`fetch_html_content(session, url, params)`:**
    *   Asynchronicznie pobiera zawartoÅ›Ä‡ HTML strony, uÅ¼ywajÄ…c losowego User-Agenta z predefiniowanej listy.
    *   ObsÅ‚uguje timeouty i bÅ‚Ä™dy HTTP.
-   **`parse_single_item_html(item_html_str, query_for_item)`:**
    *   Funkcja uruchamiana w osobnym procesie (przez `ProcessPoolExecutor`).
    *   Parsuje fragment HTML pojedynczej oferty za pomocÄ… `BeautifulSoup`.
    *   WyodrÄ™bnia tytuÅ‚, cenÄ™ (tekstowÄ… i numerycznÄ… - `_extract_price_static`), link, informacje o wysyÅ‚ce i lokalizacji.
    *   Generuje unikalne `_id` dla oferty.
    *   Zwraca sÅ‚ownik z danymi.
-   **`_sanitize_collection_name(self, name_str)`:**
    *   Normalizuje nazwÄ™ zapytania (maÅ‚e litery, spacje na `_`, usuwanie znakÃ³w specjalnych) do uÅ¼ycia jako nazwa kolekcji MongoDB.

#### `engine_api.py`:
-   **`start_scraping_endpoint()`:**
    *   Endpoint Flask API (`/start-scraping`, metoda POST) nasÅ‚uchujÄ…cy na Å¼Ä…dania od interfejsu uÅ¼ytkownika.
    *   Odbiera parametry wyszukiwania (query, ceny, sortowanie) w formacie JSON.
    *   WywoÅ‚uje `run_ebay_scraper` z `scraper.py`.
    *   Zwraca odpowiedÅº JSON ze statusem operacji i liczbÄ… znalezionych ofert.
-   **Inicjalizacja serwera Flask:**
    *   `app.run(host='0.0.0.0', port=5001, ...)`: Uruchamia serwer Flask API silnika na porcie 5001, dostÄ™pny dla innych kontenerÃ³w w sieci Docker.

#### `Dockerfile` (dla `engine`):
-   Bazuje na obrazie `python:3.10-slim`.
-   Kopiuje `requirements.txt` i instaluje zaleÅ¼noÅ›ci.
-   Kopiuje kod aplikacji silnika.
-   Eksponuje port `5001`.
-   Uruchamia `engine_api.py` przy starcie kontenera.

### ModuÅ‚ Interfejsu UÅ¼ytkownika (`flask_app/`)

#### `app.py`:
-   **Inicjalizacja i konfiguracja:**
    *   `MONGO_URI`, `DB_NAME`, `ENGINE_API_URL`: odczytywane ze zmiennych Å›rodowiskowych.
-   **`trigger_scrape()`:**
    *   Endpoint Flask (`/trigger-scrape`, metoda POST).
    *   Odbiera parametry od uÅ¼ytkownika (z JavaScriptu na stronie `index.html`) w formacie JSON.
    *   Waliduje dane wejÅ›ciowe (np. czy ceny sÄ… poprawne, czy query nie jest puste).
    *   WysyÅ‚a Å¼Ä…danie POST (bibliotekÄ… `requests`) do API silnika scrapujÄ…cego (`ENGINE_API_URL`).
    *   ObsÅ‚uguje rÃ³Å¼ne rodzaje bÅ‚Ä™dÃ³w komunikacji z silnikiem (Timeout, ConnectionError, HTTPError).
    *   Zwraca odpowiedÅº od silnika (lub wÅ‚asny komunikat bÅ‚Ä™du) do JavaScriptu.
-   **`show_results()`:**
    *   Endpoint Flask (`/results`, metoda GET).
    *   Odpowiedzialny za wyÅ›wietlenie wynikÃ³w scrapowania.
    *   Odczytuje parametry `query` i `sort` z URL.
    *   ÅÄ…czy siÄ™ z MongoDB (`get_db_client()`, `sanitize_collection_name_for_ui`).
    *   Pobiera dane z odpowiedniej kolekcji, sortujÄ…c je zgodnie z Å¼yczeniem uÅ¼ytkownika.
    *   Renderuje szablon `results.html`, przekazujÄ…c listÄ™ ofert.
-   **`index()` (implicite dla `/`):**
    *   Renderuje `index.html`.

#### `templates/index.html`:
-   Zawiera formularz HTML do wprowadzania parametrÃ³w wyszukiwania (sÅ‚owo kluczowe, ceny, typ sortowania).
-   **Skrypt JavaScript:**
    *   ObsÅ‚uguje zdarzenie `submit` formularza.
    *   Przeprowadza walidacjÄ™ po stronie klienta.
    *   WysyÅ‚a asynchroniczne Å¼Ä…danie (`fetch`) do endpointu `/trigger-scrape` w `app.py` z danymi z formularza w formacie JSON.
    *   WyÅ›wietla animacjÄ™ Å‚adowania (`loader`) i komunikaty o statusie operacji (np. "Rozpoczynam scrapowanie...", "Scraping completed successfully...") w `statusMessageDiv`.
    *   Aktualizuje link do strony wynikÃ³w (`resultsLink`) po pomyÅ›lnym scrapowaniu.

#### `templates/results.html`:
-   Szablon do wyÅ›wietlania wynikÃ³w scrapowania.
-   Otrzymuje listÄ™ ofert (`ads`) od funkcji `show_results()` w `app.py`.
-   Iteruje po liÅ›cie ofert (`{% for ad in ads %}`) i generuje tabelÄ™ HTML z danymi kaÅ¼dej oferty (tytuÅ‚, cena, wysyÅ‚ka, lokalizacja, link).
-   WyÅ›wietla informacjÄ™ o bÅ‚Ä™dzie poÅ‚Ä…czenia z bazÄ… lub braku wynikÃ³w.

#### `Dockerfile` (dla `flask_app`):
-   Podobnie jak Dockerfile silnika, bazuje na `python:3.10-slim`.
-   Instaluje zaleÅ¼noÅ›ci z `requirements.txt`.
-   Kopiuje kod aplikacji UI.
-   Eksponuje port `5000`.
-   Uruchamia `app.py` (serwer Flask UI) przy starcie kontenera.

## ğŸ–¼ï¸ Interfejs uÅ¼ytkownika - przepÅ‚yw

1.  **Strona gÅ‚Ã³wna (`index.html`):** UÅ¼ytkownik widzi formularz:
    ![Formularz na stronie gÅ‚Ã³wnej](./img/1.png)
    *   Wprowadza "SÅ‚owo kluczowe" (np. "rtx 5080").
    *   Opcjonalnie "Cena minimalna ($)" i "Cena maksymalna ($)".
    *   Wybiera "Sortuj wedÅ‚ug ceny produktu" (domyÅ›lnie "MalejÄ…co").
    *   Klika "Uruchom Scrapowanie".

2.  **Proces scrapowania:** Aplikacja wyÅ›wietla komunikat "Rozpoczynam scrapowanie eBay..." oraz animacjÄ™ Å‚adowania:

    ![Proces scrapowania - komunikat i Å‚adowanie](./img/2.png)

4.  **ZakoÅ„czenie scrapowania:** WyÅ›wietlany jest komunikat o wyniku, np. "Scraping completed successfully Znaleziono ogÅ‚oszeÅ„: 20.". Pod komunikatem aktywny staje siÄ™ link "Zobacz Wyniki":
    ![ZakoÅ„czenie scrapowania - komunikat o sukcesie](./img/3.png)

5.  **Strona wynikÃ³w (`results.html`):** Po klikniÄ™ciu "Zobacz Wyniki", uÅ¼ytkownik jest przenoszony na stronÄ™ z tabelarycznym zestawieniem znalezionych ofert. WyÅ›wietlane sÄ…: TytuÅ‚, Cena (tekst), Cena (numeryczna), WysyÅ‚ka, Lokalizacja, Link do eBay:
    ![Strona wynikÃ³w - lista ofert](./img/4.png)
    
    Wyniki mogÄ… byÄ‡ sortowane rosnÄ…co lub malejÄ…co:
    
    ![Strona wynikÃ³w - sortowanie rosnÄ…ce](./img/5.png)

## ğŸ—„ï¸ Baza danych MongoDB

-   **System:** MongoDB.
-   **Nazwa bazy danych:** `scraper_db` (zdefiniowana w `DB_NAME`).
-   **Kolekcje:** Dla kaÅ¼dego unikalnego zapytania (sÅ‚owa kluczowego) tworzona jest nowa kolekcja. Nazwa kolekcji jest generowana przez sanitizacjÄ™ zapytania (np. "rtx 5080" staje siÄ™ "rtx_5080"). Jest to realizowane przez funkcjÄ™ `_sanitize_collection_name` w `scraper.py`.
-   **Struktura dokumentu:** Opisana w sekcji "Zakres zbieranych danych".

### Logowanie do bazy danych (z terminala hosta):

Aby poÅ‚Ä…czyÄ‡ siÄ™ z bazÄ… danych uruchomionÄ… w kontenerze Dockera i zweryfikowaÄ‡ dane:

1.  **WejÅ›cie do kontenera MongoDB:**
    ```bash
    docker exec -it mongo_db_container bash
    ```
    (Gdzie `mongo_db_container` to nazwa kontenera zdefiniowana w `docker-compose.yml`).

2.  **Uruchomienie konsoli `mongosh` wewnÄ…trz kontenera:**
    ```bash
    mongosh
    ```

3.  **Operacje w konsoli `mongosh`:**
    *   WyÅ›wietlenie dostÄ™pnych baz danych:
        ```javascript
        show dbs
        ```
    *   PrzeÅ‚Ä…czenie siÄ™ do wÅ‚aÅ›ciwej bazy:
        ```javascript
        use scraper_db
        ```
    *   WyÅ›wietlenie dostÄ™pnych kolekcji (np. po wyszukaniu "rtx 5080"):
        ```javascript
        show collections
        ```
        (Powinna pojawiÄ‡ siÄ™ kolekcja `rtx_5080`).
    *   PodglÄ…d wszystkich dokumentÃ³w w kolekcji (screen "WyÅ›wietlenie wszystkich dokumentÃ³w z kolekcji rtx_5080"):
        ```javascript
        db.rtx_5080.find();
        ```
    *   PodglÄ…d jednego dokumentu:
        ```javascript
        db.rtx_5080.findOne();
        ```
## ğŸ§¾ Autorzy

- **Oliwier BogdaÅ„ski** â€“ 21181  
- **Kacper Szponar** â€“ 21306

Repozytorium GitHub: [https://github.com/Thiago1717/PRIR-Projekt](https://github.com/Thiago1717/PRIR-Projekt)
